---
title: ElatoAI - Realtime Speech AI for ESP32
description: Bring OpenAI, Gemini, ElevenLabs, and Hume AI voices to ESP32 hardware with realtime speech-to-speech conversations
authorIds:
  - akashdeep-deb
categories:
  - agents
  - voices
  - text-to-speech
  - speech-to-text
isFeatured: false
date: "2025-10-30"
image: /images/11-labs-arduino-esp32.png
demoUrl: https://www.elatoai.com/
repoUrl: https://github.com/akdeb/ElatoAI
videoUrl: https://youtu.be/7LKTIuEW-hg
xUrl: https://x.com/akad3b
---

# ElatoAI: Realtime Speech AI Agents for ESP32

ElatoAI brings state-of-the-art realtime AI speech to ESP32 microcontrollers, enabling uninterrupted conversations for over 15 minutes using **ElevenLabs Conversational AI Agents**, OpenAI Realtime API, Gemini Live API, and Hume AI EVI-4. This open-source project combines secure WebSockets, Deno Edge Functions, and optimized Arduino firmware to create a complete voice AI platform for IoT devices.

## Overview

ElatoAI is a complete hardware and software solution that transforms ESP32 microcontrollers into intelligent voice-enabled devices. The project supports multiple AI providers, with ElevenLabs Conversational AI Agents providing industry-leading voice quality and natural conversation flows. Whether you're building smart toys, voice assistants, or interactive installations, ElatoAI provides the infrastructure for production-ready voice AI on embedded hardware.

The system is designed with global scalability in mind, using edge computing to maintain low latency (<2s round-trip) anywhere in the world, making it perfect for commercial products and hobbyist projects alike.

## Key Features

- **Multi-Provider Support**: Seamlessly switch between OpenAI Realtime API, Gemini Live API, **ElevenLabs Conversational AI Agents**, and Hume AI EVI-4
- **Custom AI Agents**: Create personalized characters with unique personalities and voices through a Next.js web interface
- **Production-Ready Hardware**: DIY PCB design with optimized audio processing, no PSRAM required
- **Secure Global Infrastructure**: Encrypted WebSocket connections via Deno Edge Functions for sub-2s latency worldwide
- **Extended Conversations**: Support for 15+ minute uninterrupted conversations with server-side VAD turn detection
- **High-Quality Audio**: Opus codec compression at 12kbps with 24kHz sampling rate for crystal-clear voice
- **Complete Device Management**: Web-based control panel for volume, OTA updates, factory reset, and device registration
- **Realtime Transcripts**: All conversations stored with transcripts in Supabase for analysis and debugging
- **WiFi Captive Portal**: Easy network configuration directly from the ESP32 device
- **Advanced Voice Controls**: Pitch factor adjustment for cartoon-like voices, tool calling for agentic behaviors
- **Touch & Button Controls**: Flexible input options including tap-to-wake functionality
- **OAuth Integration**: Secure user authentication for managing multiple devices and AI characters

## How It Works

ElatoAI uses a three-tier architecture optimized for low latency and global scalability:

### Architecture Components

1. **Next.js Frontend (Vercel)**: Web interface for creating AI agents, managing devices, and conducting voice conversations via WebRTC. Users can customize agent personalities, select voices, and control hardware settings in real-time.

2. **Deno Edge Functions (Deno Deploy/Supabase Edge)**: Lightweight serverless functions that handle WebSocket connections between ESP32 devices and AI provider APIs. The edge deployment ensures minimal latency regardless of geographic location.

3. **ESP32 IoT Client (Arduino/PlatformIO)**: Optimized firmware that handles microphone input, Opus audio encoding/decoding, WebSocket communication, and speaker output. The firmware manages connection lifecycle, audio buffering, and device controls.

### Data Flow

```
User Speech â†’ ESP32 Microphone â†’ Opus Encoding â†’ WebSocket (Secure) 
â†’ Deno Edge Function â†’ AI Provider (ElevenLabs/OpenAI/Gemini/Hume) 
â†’ Audio Response â†’ Edge Function â†’ WebSocket â†’ ESP32 â†’ Opus Decoding 
â†’ Speaker â†’ User Hears AI
```

The system uses server-side Voice Activity Detection (VAD) to manage conversation turn-taking, ensuring natural dialogue flow without the need for complex edge processing on the microcontroller.

## Technologies Used

### ElevenLabs Integration
- **Conversational AI Agents**: Natural, expressive voice synthesis with emotional intelligence
- **Voice Library**: Access to ElevenLabs' extensive voice collection for character creation
- **Low-Latency Streaming**: Optimized for real-time conversations with minimal delay

### Core Stack
- **Frontend**: Next.js 14, React 18, Vercel deployment, WebRTC for browser-based conversations
- **Backend**: Supabase (PostgreSQL with Row-Level Security), Supabase Auth for OAuth
- **Edge Computing**: Deno runtime for ultra-low latency serverless functions
- **IoT Firmware**: PlatformIO, Arduino Framework for ESP32-S3
- **Audio Processing**: Opus codec, arduino-audio-tools, arduino-libopus
- **Communication**: Secure WebSockets (WSS), WebSockets library for Arduino
- **Hardware**: ESP32-S3 DevKit, custom PCB design with microphone and speaker integration

### Key Libraries
- ArduinoJson for efficient data serialization
- ESPAsyncWebServer for WiFi configuration
- ESP32_Button for touch and button controls
- WebSockets library for real-time communication

## Getting Started

### Prerequisites
- ESP32-S3 development board (or custom ElatoAI hardware)
- Node.js 22.13.0+
- Deno runtime
- Supabase CLI and Docker Desktop
- PlatformIO or Arduino IDE
- ElevenLabs API key ([get one here](https://elevenlabs.io/))

### Quick Setup

```bash
# 1. Clone the repository
git clone git@github.com:akdeb/ElatoAI.git
cd ElatoAI

# 2. Start local Supabase backend
brew install supabase/tap/supabase
supabase start

# 3. Set up Next.js frontend
cd frontend-nextjs
npm install
cp .env.example .env.local
# Add your Supabase and API keys to .env.local
npm run dev

# 4. Run Deno edge server (for local development)
cd ../server-deno
cp .env.example .env
# Add your ElevenLabs API key and other provider keys to .env
deno run -A --env-file=.env main.ts

# 5. Flash ESP32 firmware
cd ../firmware-arduino
# Configure Config.cpp with your local IP
# Upload to ESP32 via PlatformIO

# 6. Connect ESP32 to WiFi
# Device creates "ELATO-DEVICE" captive portal
# Navigate to http://192.168.4.1 to configure

# 7. Start talking!
# Access web interface at localhost:3000
# Login with: admin@elatoai.com / admin
```

### Using ElevenLabs Conversational AI

To configure ElevenLabs as your voice provider:

1. Get your ElevenLabs API key from [elevenlabs.io](https://elevenlabs.io/)
2. Add to your `.env` file: `ELEVENLABS_API_KEY=your_key_here`
3. In the web interface, select ElevenLabs as your AI provider
4. Choose from pre-configured voices or create custom voice agents
5. Start your conversation - the ESP32 will stream audio through ElevenLabs' API

See the [ElevenLabs integration docs](https://github.com/akdeb/ElatoAI/blob/main/docs/ElevenLabs.md) for advanced configuration options.

## Production Deployment

ElatoAI offers two deployment modes:

- **Hosted Mode**: Free tier includes 30 minutes/month of edge server usage. Register your device at [elatoai.com/home/settings/device](https://www.elatoai.com/home/settings/device)
- **Self-Hosted**: Deploy your own Deno edge functions on Deno Deploy or Supabase Edge Functions for unlimited usage

## Hardware Design

The project includes complete DIY hardware designs with PCB schematics optimized for audio quality and power efficiency. The hardware features:

- ESP32-S3 microcontroller (no PSRAM required)
- High-quality MEMS microphone
- Class D amplifier for speaker output
- Power management circuitry
- Touch sensor integration
- LED status indicators

Check out the [hardware documentation](https://github.com/akdeb/ElatoAI) for full schematics and assembly instructions.

## Use Cases

- **Interactive Toys**: Create toys that talk with unique AI personalities (featured on [Kickstarter](https://www.kickstarter.com/projects/elatoai/elato-make-toys-talk-with-ai-voices))
- **Voice Assistants**: Build custom voice assistants for home automation
- **Educational Tools**: Language learning devices with real-time pronunciation feedback
- **Art Installations**: Interactive exhibits with conversational AI characters
- **Accessibility Devices**: Voice-enabled tools for users with visual or motor impairments

## Community & Support

- **Discord**: Join 115+ members at [discord.gg/KJWxDPBRUj](https://discord.gg/KJWxDPBRUj)
- **Documentation**: Full guides at [elatoai.com](https://www.elatoai.com/)
- **Video Tutorials**: [YouTube channel](https://youtube.com/watch?v=7LKTIuEW-hg) with setup guides
- **OpenAI Cookbook**: Featured in [OpenAI's official examples](https://cookbook.openai.com/examples/voice_solutions/running_realtime_api_speech_on_esp32_arduino_edge_runtime_elatoai)

## Performance Metrics

- âš¡ï¸ **Latency**: <2s round-trip globally via edge computing
- ðŸŽ§ **Audio Quality**: 24kHz Opus codec at 12kbps
- â³ **Conversation Length**: 15+ minutes continuous (tested up to 17 minutes)
- ðŸŒŽ **Global Coverage**: Optimized for worldwide deployment
- ðŸ”’ **Security**: AES-256 encryption, WSS protocol, Postgres RLS

## Contributing

We welcome contributions! Priority areas:

- Speech interruption detection on ESP32
- Azure OpenAI integration
- Cartesia API support
- Arduino IDE compatibility
- MCP (Model Context Protocol) support

---

**Featured in**: OpenAI Cookbook | Hacker News | Adafruit Blog

**License**: MIT - Free for commercial and personal use

If you find this project useful, please â­ star the repository and check out our hardware offerings at [elatoai.com](https://www.elatoai.com/)!

